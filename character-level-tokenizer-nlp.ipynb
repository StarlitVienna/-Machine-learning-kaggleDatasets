{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/evelynartoria/character-level-tokenizer-nlp?scriptVersionId=187820972\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"083e4db0","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-11T12:03:04.918595Z","iopub.status.busy":"2024-07-11T12:03:04.918179Z","iopub.status.idle":"2024-07-11T12:03:06.105802Z","shell.execute_reply":"2024-07-11T12:03:06.10378Z"},"papermill":{"duration":1.19663,"end_time":"2024-07-11T12:03:06.108991","exception":false,"start_time":"2024-07-11T12:03:04.912361","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/wikipedia-sentences/wikisent2.txt\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"id":"a0a5b01e","metadata":{"execution":{"iopub.execute_input":"2024-07-11T12:03:06.1211Z","iopub.status.busy":"2024-07-11T12:03:06.120041Z","iopub.status.idle":"2024-07-11T12:03:06.137288Z","shell.execute_reply":"2024-07-11T12:03:06.134735Z"},"papermill":{"duration":0.026833,"end_time":"2024-07-11T12:03:06.141258","exception":false,"start_time":"2024-07-11T12:03:06.114425","status":"completed"},"tags":[]},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self, text_file_path):\n","        self.text = \"\"\n","        self.lines = []\n","        self.vocab = []\n","        \n","        with open(text_file_path, 'r', encoding=\"utf-8\") as f:\n","            self.text = f.read()\n","            self.lines = self.text.splitlines()\n","            self.vocab = sorted(set(self.text))\n","            \n","        self.stoi = {c: v for v, c in enumerate(self.vocab)}\n","        self.itos = {v: c for c, v in self.stoi.items()}\n","        \n","    \n","    @property\n","    def vocab_size(self):\n","        return len(self.vocab)\n","    \n","    @property\n","    def info_dict(self):\n","        info_dict = {\n","            \"vocab\": self.vocab,\n","            \"vocab_size\": self.vocab_size\n","        }\n","        \n","        return info_dict\n","    \n","    def encode(self, text):\n","        return [self.stoi[ch] for ch in text]\n","    \n","    def decode(self, tokens):\n","        return \"\".join([self.itos[idx] for idx in tokens])\n","    \n","        "]},{"cell_type":"code","execution_count":3,"id":"53e270b3","metadata":{"execution":{"iopub.execute_input":"2024-07-11T12:03:06.153106Z","iopub.status.busy":"2024-07-11T12:03:06.152595Z","iopub.status.idle":"2024-07-11T12:03:36.463863Z","shell.execute_reply":"2024-07-11T12:03:36.462162Z"},"papermill":{"duration":30.321608,"end_time":"2024-07-11T12:03:36.46701","exception":false,"start_time":"2024-07-11T12:03:06.145402","status":"completed"},"tags":[]},"outputs":[],"source":["tokenizer = Tokenizer(\"/kaggle/input/wikipedia-sentences/wikisent2.txt\")"]},{"cell_type":"code","execution_count":4,"id":"d547892d","metadata":{"execution":{"iopub.execute_input":"2024-07-11T12:03:36.476966Z","iopub.status.busy":"2024-07-11T12:03:36.476496Z","iopub.status.idle":"2024-07-11T12:03:36.484825Z","shell.execute_reply":"2024-07-11T12:03:36.482828Z"},"papermill":{"duration":0.016883,"end_time":"2024-07-11T12:03:36.487983","exception":false,"start_time":"2024-07-11T12:03:36.4711","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tokenizer vocab size is --> 96\n","tokenizer vocab is --> ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n"]}],"source":["print(f\"tokenizer vocab size is --> {tokenizer.vocab_size}\")\n","print(f\"tokenizer vocab is --> {tokenizer.vocab}\")"]},{"cell_type":"code","execution_count":5,"id":"821d27c0","metadata":{"execution":{"iopub.execute_input":"2024-07-11T12:03:36.497944Z","iopub.status.busy":"2024-07-11T12:03:36.497497Z","iopub.status.idle":"2024-07-11T12:03:36.503897Z","shell.execute_reply":"2024-07-11T12:03:36.502354Z"},"papermill":{"duration":0.015207,"end_time":"2024-07-11T12:03:36.507342","exception":false,"start_time":"2024-07-11T12:03:36.492135","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tokenizer information dictionary: \n"," {'vocab': ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~'], 'vocab_size': 96}\n"]}],"source":["print(f\"tokenizer information dictionary: \\n {tokenizer.info_dict}\")"]},{"cell_type":"code","execution_count":6,"id":"71a636c8","metadata":{"execution":{"iopub.execute_input":"2024-07-11T12:03:36.517652Z","iopub.status.busy":"2024-07-11T12:03:36.517233Z","iopub.status.idle":"2024-07-11T12:03:36.524977Z","shell.execute_reply":"2024-07-11T12:03:36.523587Z"},"papermill":{"duration":0.01672,"end_time":"2024-07-11T12:03:36.528013","exception":false,"start_time":"2024-07-11T12:03:36.511293","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["encoded text:\n","[34, 67, 80, 86, 85, 1, 85, 73, 74, 84, 1, 71, 74, 77, 70, 0, 0, 36, 80, 77, 77, 70, 68, 85, 74, 80, 79, 1, 80, 71, 1, 24, 15, 25, 1, 78, 74, 77, 77, 74, 80, 79, 1, 84, 70, 79, 85, 70, 79, 68, 70, 84, 1, 9, 80, 79, 70, 1, 81, 70, 83, 1, 77, 74, 79, 70, 10, 1, 71, 83, 80, 78, 1, 34, 86, 72, 86, 84, 85, 1, 19, 17, 18, 25, 1, 38, 79, 72, 77, 74, 84, 73, 1, 88, 74, 76, 74, 81, 70, 69, 74, 66, 1, 69, 86, 78, 81, 15, 1, 53, 73, 70, 84, 70, 1, 66, 83, 70, 1, 80, 79, 77, 90, 1, 84, 70, 79, 85, 70, 79, 68, 70, 84, 1, 71, 80, 86, 79, 69, 1, 74, 79, 1, 85, 73, 70, 1, 80, 81, 70, 79, 74, 79, 72, 1, 85, 70, 89, 85, 1, 80, 71, 1, 68, 80, 79, 85, 70, 79, 85, 1, 81, 66, 72, 70, 84, 15, 1, 39, 74, 77, 85, 70, 83, 74, 79, 72, 1, 66, 81, 81, 77, 74, 70, 69, 1, 85, 80, 1, 83, 70, 78, 80, 87, 70, 1, 75, 86, 79, 76, 1, 84, 70, 79, 85, 70, 79, 68, 70, 84, 13, 1, 67, 86, 85, 1, 84, 80, 78, 70, 1, 81, 80, 80, 83, 77, 90, 1, 71, 80, 83, 78, 70, 69, 1, 84, 70, 79, 85, 70, 79, 68, 70, 84, 1, 78, 74, 72, 73, 85, 1, 84, 85, 74, 77, 77, 1, 70, 89, 74, 84, 85, 15]\n","\n","\n","decoded text:\n","About this file\n","\n","Collection of 7.8 million sentences (one per line) from August 2018 English wikipedia dump. These are only sentences found in the opening text of content pages. Filtering applied to remove junk sentences, but some poorly formed sentences might still exist.\n","\n","\n"]}],"source":["test_text = (\n","    \"\"\"About this file\n","\n","Collection of 7.8 million sentences (one per line) from August 2018 English wikipedia dump. These are only sentences found in the opening text of content pages. Filtering applied to remove junk sentences, but some poorly formed sentences might still exist.\"\"\"\n",")\n","encoded = tokenizer.encode(test_text)\n","decoded = tokenizer.decode(encoded)\n","\n","print(f\"encoded text:\\n{encoded}\\n\\n\")\n","print(f\"decoded text:\\n{decoded}\\n\\n\")"]},{"cell_type":"code","execution_count":null,"id":"58676adf","metadata":{"papermill":{"duration":0.003621,"end_time":"2024-07-11T12:03:36.535563","exception":false,"start_time":"2024-07-11T12:03:36.531942","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":46601,"sourceId":84740,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":36.914648,"end_time":"2024-07-11T12:03:37.764498","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-11T12:03:00.84985","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}