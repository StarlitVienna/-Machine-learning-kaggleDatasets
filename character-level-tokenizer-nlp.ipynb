{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/evelynartoria/character-level-tokenizer-nlp?scriptVersionId=187845008\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"5f9bdef9","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-11T14:51:41.401136Z","iopub.status.busy":"2024-07-11T14:51:41.400717Z","iopub.status.idle":"2024-07-11T14:51:42.415221Z","shell.execute_reply":"2024-07-11T14:51:42.41361Z"},"papermill":{"duration":1.02351,"end_time":"2024-07-11T14:51:42.418175","exception":false,"start_time":"2024-07-11T14:51:41.394665","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/wikipedia-sentences/wikisent2.txt\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"efac22c6","metadata":{"papermill":{"duration":0.003857,"end_time":"2024-07-11T14:51:42.426628","exception":false,"start_time":"2024-07-11T14:51:42.422771","status":"completed"},"tags":[]},"source":["# Introduction\n","- Just a simple character tokenizer class for natural language processing tasks"]},{"cell_type":"code","execution_count":2,"id":"cfa19b69","metadata":{"execution":{"iopub.execute_input":"2024-07-11T14:51:42.437201Z","iopub.status.busy":"2024-07-11T14:51:42.436542Z","iopub.status.idle":"2024-07-11T14:51:42.454817Z","shell.execute_reply":"2024-07-11T14:51:42.453663Z"},"papermill":{"duration":0.026934,"end_time":"2024-07-11T14:51:42.457763","exception":false,"start_time":"2024-07-11T14:51:42.430829","status":"completed"},"tags":[]},"outputs":[],"source":["# Class to make a character level tokenizer from text\n","class Tokenizer:\n","    def __init__(self, text_file_path):\n","        self.text = \"\"\n","        self.lines = []\n","        self.special_characters = [\"<SOS>\", \"<EOS>\", \"<PAD>\"]\n","        self.vocab = self.special_characters\n","        \n","        with open(text_file_path, 'r', encoding=\"utf-8\") as f:\n","            self.text = f.read()[:1000000]\n","            self.lines = self.text.splitlines()\n","            self.vocab = sorted(set(self.text)) + self.vocab\n","            \n","        self.stoi = {c: v for v, c in enumerate(self.vocab)}\n","        self.itos = {v: c for c, v in self.stoi.items()}\n","        \n","    \n","    @property\n","    def max_len(self):\n","        return max([len(line) for line in self.lines])\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.vocab)\n","    \n","    @property\n","    def info_dict(self):\n","        info_dict = {\n","            \"vocab\": self.vocab,\n","            \"vocab_size\": self.vocab_size\n","        }\n","        \n","        return info_dict\n","    \n","    def encode(self, text: str, add_special_characters: bool = False, padding: bool = False):\n","        output = [self.stoi[ch] for ch in text]\n","        if add_special_characters and padding:\n","            n_pads = self.max_len - len(text)\n","            return [self.stoi[\"<SOS>\"]] + [self.stoi[ch] for ch in text] + [self.stoi[\"<EOS>\"]] + [self.stoi[\"<PAD>\"]]*n_pads\n","        elif add_special_characters:\n","            return [self.stoi[\"<SOS>\"]] + [self.stoi[ch] for ch in text] + [self.stoi[\"<EOS>\"]]\n","        else:\n","            return [self.stoi[ch] for ch in text]\n","\n","        \n","    \n","    def decode(self, tokens, remove_special_tokens: bool = True):\n","        if remove_special_tokens:\n","            encoded_specials_characters = [self.stoi[ch] for ch in self.special_characters]\n","            return \"\".join([self.itos[token] for token in tokens if token not in encoded_specials_characters])\n","\n","        return \"\".join([self.itos[token] for token in tokens])"]},{"cell_type":"code","execution_count":3,"id":"138c7f29","metadata":{"execution":{"iopub.execute_input":"2024-07-11T14:51:42.468051Z","iopub.status.busy":"2024-07-11T14:51:42.467617Z","iopub.status.idle":"2024-07-11T14:51:52.697438Z","shell.execute_reply":"2024-07-11T14:51:52.696309Z"},"papermill":{"duration":10.238216,"end_time":"2024-07-11T14:51:52.70038","exception":false,"start_time":"2024-07-11T14:51:42.462164","status":"completed"},"tags":[]},"outputs":[],"source":["tokenizer = Tokenizer(\"/kaggle/input/wikipedia-sentences/wikisent2.txt\")"]},{"cell_type":"code","execution_count":4,"id":"e610ec0b","metadata":{"execution":{"iopub.execute_input":"2024-07-11T14:51:52.711164Z","iopub.status.busy":"2024-07-11T14:51:52.710734Z","iopub.status.idle":"2024-07-11T14:51:52.716805Z","shell.execute_reply":"2024-07-11T14:51:52.715648Z"},"papermill":{"duration":0.014846,"end_time":"2024-07-11T14:51:52.71968","exception":false,"start_time":"2024-07-11T14:51:52.704834","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tokenizer vocab size is --> 91\n","tokenizer vocab is --> ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '}', '<SOS>', '<EOS>', '<PAD>']\n"]}],"source":["print(f\"tokenizer vocab size is --> {tokenizer.vocab_size}\")\n","print(f\"tokenizer vocab is --> {tokenizer.vocab}\")"]},{"cell_type":"code","execution_count":5,"id":"8828ff44","metadata":{"execution":{"iopub.execute_input":"2024-07-11T14:51:52.730085Z","iopub.status.busy":"2024-07-11T14:51:52.729667Z","iopub.status.idle":"2024-07-11T14:51:52.736002Z","shell.execute_reply":"2024-07-11T14:51:52.734801Z"},"papermill":{"duration":0.014551,"end_time":"2024-07-11T14:51:52.7386","exception":false,"start_time":"2024-07-11T14:51:52.724049","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tokenizer information dictionary: \n"," {'vocab': ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '}', '<SOS>', '<EOS>', '<PAD>'], 'vocab_size': 91}\n"]}],"source":["print(f\"tokenizer information dictionary: \\n {tokenizer.info_dict}\")"]},{"cell_type":"code","execution_count":6,"id":"6d0117a4","metadata":{"execution":{"iopub.execute_input":"2024-07-11T14:51:52.749465Z","iopub.status.busy":"2024-07-11T14:51:52.749059Z","iopub.status.idle":"2024-07-11T14:51:52.757469Z","shell.execute_reply":"2024-07-11T14:51:52.755725Z"},"papermill":{"duration":0.017361,"end_time":"2024-07-11T14:51:52.760606","exception":false,"start_time":"2024-07-11T14:51:52.743245","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["encoded text:\n","[32, 62, 75, 81, 80, 1, 80, 68, 69, 79, 1, 66, 69, 72, 65, 0, 0, 34, 75, 72, 72, 65, 63, 80, 69, 75, 74, 1, 75, 66, 1, 24, 15, 25, 1, 73, 69, 72, 72, 69, 75, 74, 1, 79, 65, 74, 80, 65, 74, 63, 65, 79, 1, 9, 75, 74, 65, 1, 76, 65, 78, 1, 72, 69, 74, 65, 10, 1, 66, 78, 75, 73, 1, 32, 81, 67, 81, 79, 80, 1, 19, 17, 18, 25, 1, 36, 74, 67, 72, 69, 79, 68, 1, 83, 69, 71, 69, 76, 65, 64, 69, 61, 1, 64, 81, 73, 76, 15, 1, 51, 68, 65, 79, 65, 1, 61, 78, 65, 1, 75, 74, 72, 85, 1, 79, 65, 74, 80, 65, 74, 63, 65, 79, 1, 66, 75, 81, 74, 64, 1, 69, 74, 1, 80, 68, 65, 1, 75, 76, 65, 74, 69, 74, 67, 1, 80, 65, 84, 80, 1, 75, 66, 1, 63, 75, 74, 80, 65, 74, 80, 1, 76, 61, 67, 65, 79, 15, 1, 37, 69, 72, 80, 65, 78, 69, 74, 67, 1, 61, 76, 76, 72, 69, 65, 64, 1, 80, 75, 1, 78, 65, 73, 75, 82, 65, 1, 70, 81, 74, 71, 1, 79, 65, 74, 80, 65, 74, 63, 65, 79, 13, 1, 62, 81, 80, 1, 79, 75, 73, 65, 1, 76, 75, 75, 78, 72, 85, 1, 66, 75, 78, 73, 65, 64, 1, 79, 65, 74, 80, 65, 74, 63, 65, 79, 1, 73, 69, 67, 68, 80, 1, 79, 80, 69, 72, 72, 1, 65, 84, 69, 79, 80, 15]\n","\n","\n","decoded text:\n","About this file\n","\n","Collection of 7.8 million sentences (one per line) from August 2018 English wikipedia dump. These are only sentences found in the opening text of content pages. Filtering applied to remove junk sentences, but some poorly formed sentences might still exist.\n","\n","\n"]}],"source":["test_text = (\n","    \"\"\"About this file\n","\n","Collection of 7.8 million sentences (one per line) from August 2018 English wikipedia dump. These are only sentences found in the opening text of content pages. Filtering applied to remove junk sentences, but some poorly formed sentences might still exist.\"\"\"\n",")\n","encoded = tokenizer.encode(test_text)\n","decoded = tokenizer.decode(encoded)\n","\n","print(f\"encoded text:\\n{encoded}\\n\\n\")\n","print(f\"decoded text:\\n{decoded}\\n\\n\")"]},{"cell_type":"code","execution_count":7,"id":"2d0d8600","metadata":{"execution":{"iopub.execute_input":"2024-07-11T14:51:52.772068Z","iopub.status.busy":"2024-07-11T14:51:52.771626Z","iopub.status.idle":"2024-07-11T14:51:52.780497Z","shell.execute_reply":"2024-07-11T14:51:52.779116Z"},"papermill":{"duration":0.0191,"end_time":"2024-07-11T14:51:52.784319","exception":false,"start_time":"2024-07-11T14:51:52.765219","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["encoded text:\n","[88, 32, 62, 75, 81, 80, 1, 80, 68, 69, 79, 1, 66, 69, 72, 65, 0, 0, 34, 75, 72, 72, 65, 63, 80, 69, 75, 74, 1, 75, 66, 1, 24, 15, 25, 1, 73, 69, 72, 72, 69, 75, 74, 1, 79, 65, 74, 80, 65, 74, 63, 65, 79, 1, 9, 75, 74, 65, 1, 76, 65, 78, 1, 72, 69, 74, 65, 10, 1, 66, 78, 75, 73, 1, 32, 81, 67, 81, 79, 80, 1, 19, 17, 18, 25, 1, 36, 74, 67, 72, 69, 79, 68, 1, 83, 69, 71, 69, 76, 65, 64, 69, 61, 1, 64, 81, 73, 76, 15, 1, 51, 68, 65, 79, 65, 1, 61, 78, 65, 1, 75, 74, 72, 85, 1, 79, 65, 74, 80, 65, 74, 63, 65, 79, 1, 66, 75, 81, 74, 64, 1, 69, 74, 1, 80, 68, 65, 1, 75, 76, 65, 74, 69, 74, 67, 1, 80, 65, 84, 80, 1, 75, 66, 1, 63, 75, 74, 80, 65, 74, 80, 1, 76, 61, 67, 65, 79, 15, 1, 37, 69, 72, 80, 65, 78, 69, 74, 67, 1, 61, 76, 76, 72, 69, 65, 64, 1, 80, 75, 1, 78, 65, 73, 75, 82, 65, 1, 70, 81, 74, 71, 1, 79, 65, 74, 80, 65, 74, 63, 65, 79, 13, 1, 62, 81, 80, 1, 79, 75, 73, 65, 1, 76, 75, 75, 78, 72, 85, 1, 66, 75, 78, 73, 65, 64, 1, 79, 65, 74, 80, 65, 74, 63, 65, 79, 1, 73, 69, 67, 68, 80, 1, 79, 80, 69, 72, 72, 1, 65, 84, 69, 79, 80, 15, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90]\n","\n","\n","decoded text:\n","<SOS>About this file\n","\n","Collection of 7.8 million sentences (one per line) from August 2018 English wikipedia dump. These are only sentences found in the opening text of content pages. Filtering applied to remove junk sentences, but some poorly formed sentences might still exist.<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n","\n","\n"]}],"source":["encoded = tokenizer.encode(test_text, add_special_characters=True, padding=True)\n","decoded = tokenizer.decode(encoded, remove_special_tokens=False)\n","\n","print(f\"encoded text:\\n{encoded}\\n\\n\")\n","print(f\"decoded text:\\n{decoded}\\n\\n\")"]},{"cell_type":"code","execution_count":null,"id":"68036222","metadata":{"papermill":{"duration":0.004418,"end_time":"2024-07-11T14:51:52.793502","exception":false,"start_time":"2024-07-11T14:51:52.789084","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":46601,"sourceId":84740,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":15.124855,"end_time":"2024-07-11T14:51:53.320112","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-11T14:51:38.195257","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}