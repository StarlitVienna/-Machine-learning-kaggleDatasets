{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8871363,"sourceType":"datasetVersion","datasetId":5339176}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.execute_input":"2024-07-10T15:22:24.187072Z","iopub.status.busy":"2024-07-10T15:22:24.186324Z","iopub.status.idle":"2024-07-10T15:22:24.191750Z","shell.execute_reply":"2024-07-10T15:22:24.190534Z","shell.execute_reply.started":"2024-07-10T15:22:24.187037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ngenerator = torch.Generator(device=device)\ntorch.set_default_device(device)\nprint(f\"default device set to {device}\")","metadata":{"execution":{"iopub.execute_input":"2024-07-10T15:31:50.268154Z","iopub.status.busy":"2024-07-10T15:31:50.267133Z","iopub.status.idle":"2024-07-10T15:31:50.275115Z","shell.execute_reply":"2024-07-10T15:31:50.274038Z","shell.execute_reply.started":"2024-07-10T15:31:50.268115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/shakespeare/input.txt\", 'r', encoding=\"utf-8\") as f:\n    text = f.read()","metadata":{"execution":{"iopub.execute_input":"2024-07-10T15:31:51.168883Z","iopub.status.busy":"2024-07-10T15:31:51.168074Z","iopub.status.idle":"2024-07-10T15:31:51.182995Z","shell.execute_reply":"2024-07-10T15:31:51.181975Z","shell.execute_reply.started":"2024-07-10T15:31:51.168848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = sorted(set(text))\nvocab_size = len(vocab)","metadata":{"execution":{"iopub.execute_input":"2024-07-10T15:31:51.297422Z","iopub.status.busy":"2024-07-10T15:31:51.296723Z","iopub.status.idle":"2024-07-10T15:31:51.321704Z","shell.execute_reply":"2024-07-10T15:31:51.320580Z","shell.execute_reply.started":"2024-07-10T15:31:51.297387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stoi = {c: v for v, c in enumerate(vocab)}\nitos = {v: c for c, v in stoi.items()}\n\nprint(stoi[\"h\"])\nprint(itos[46])","metadata":{"execution":{"iopub.execute_input":"2024-07-10T15:31:51.430521Z","iopub.status.busy":"2024-07-10T15:31:51.429483Z","iopub.status.idle":"2024-07-10T15:31:51.436203Z","shell.execute_reply":"2024-07-10T15:31:51.435120Z","shell.execute_reply.started":"2024-07-10T15:31:51.430482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode = lambda e: [stoi[ch] for ch in e]\ndecode = lambda d: \"\".join([itos[idx] for idx in d])\n\nencoded = encode(\"hello how are you?\")\ndecoded = decode(encoded)\n\nprint(encoded, decoded)","metadata":{"execution":{"iopub.execute_input":"2024-07-10T15:31:51.562598Z","iopub.status.busy":"2024-07-10T15:31:51.562212Z","iopub.status.idle":"2024-07-10T15:31:51.568868Z","shell.execute_reply":"2024-07-10T15:31:51.567748Z","shell.execute_reply.started":"2024-07-10T15:31:51.562567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_size = 256\nrandom_idx_tensor = torch.randperm(10000//context_size) * context_size\nprint(random_idx_tensor)","metadata":{"execution":{"iopub.execute_input":"2024-07-10T16:51:26.039712Z","iopub.status.busy":"2024-07-10T16:51:26.039310Z","iopub.status.idle":"2024-07-10T16:51:26.046797Z","shell.execute_reply":"2024-07-10T16:51:26.045650Z","shell.execute_reply.started":"2024-07-10T16:51:26.039680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(data):\n    random_idx_tensor = torch.randperm((len(data)-context_size)//context_size) * context_size\n    inputs = torch.stack([data[idx:idx+context_size] for idx in random_idx_tensor])\n    labels = torch.stack([data[idx+1:idx+context_size+1] for idx in random_idx_tensor])\n    \n    return TensorDataset(inputs.to(torch.long), labels.to(torch.long))","metadata":{"execution":{"iopub.execute_input":"2024-07-10T16:51:26.553385Z","iopub.status.busy":"2024-07-10T16:51:26.552941Z","iopub.status.idle":"2024-07-10T16:51:26.559693Z","shell.execute_reply":"2024-07-10T16:51:26.558609Z","shell.execute_reply.started":"2024-07-10T16:51:26.553338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = torch.tensor(encode(text))\ndataset = make_dataset(data=data)\nsample_input = dataset[0][0]\nsample_label = dataset[0][1]\n\nprint(sample_input)\nprint(sample_label)\n\nprint(f\"dataset length --> {len(dataset)} ({len(dataset) * context_size} characters), that is, about the length of text {len(text)} - context_size --> {len(text)-context_size}\")","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:47:49.262321Z","iopub.status.busy":"2024-07-10T17:47:49.261907Z","iopub.status.idle":"2024-07-10T17:47:55.866903Z","shell.execute_reply":"2024-07-10T17:47:55.865695Z","shell.execute_reply.started":"2024-07-10T17:47:49.262288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_split = int(len(dataset)*0.75)\ntest_split = int(len(dataset)-train_split)\n\ntrain_dataset, test_dataset = random_split(dataset=dataset, lengths=[train_split, test_split], generator=generator)","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:47:55.869741Z","iopub.status.busy":"2024-07-10T17:47:55.869029Z","iopub.status.idle":"2024-07-10T17:47:55.882328Z","shell.execute_reply":"2024-07-10T17:47:55.881209Z","shell.execute_reply.started":"2024-07-10T17:47:55.869703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, generator=generator)","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:47:55.884283Z","iopub.status.busy":"2024-07-10T17:47:55.883929Z","iopub.status.idle":"2024-07-10T17:47:55.890497Z","shell.execute_reply":"2024-07-10T17:47:55.889212Z","shell.execute_reply.started":"2024-07-10T17:47:55.884253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, n_embd, head_size, context_size):\n        super(Head, self).__init__()\n        \n        self.Q = nn.Linear(in_features=n_embd, out_features=head_size) # takes in BxTxC and return BxTxHead_size\n        self.K = nn.Linear(in_features=n_embd, out_features=head_size)\n        self.V = nn.Linear(in_features=n_embd, out_features=head_size)\n        \n        self.register_buffer(\"tril\", torch.tril(torch.ones(size=(context_size, context_size))))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, T, C = x.shape # batch_size by context_size by n_embd\n        q = self.Q(x) # BxTxHead_size\n        k = self.K(x) # BxTxHead_size\n        \n        wei = q @ k.transpose(-2, -1) * (C ** -0.5) # BxTxHead_size @ BxHead_sizexT --> BxTxT then divided by the square root of n_embd\n        \n        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # :T and :T is needed in case context is smaller than context_size\n        wei = torch.softmax(wei, dim=-1)\n        v = self.V(x) # BxTxHead_size\n        #print(f\"wei shape is {wei.shape}\")\n        output = wei @ v # BxTxT @ BxTxHead_size --> BxTxHead_size\n        \n        return output\n","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:47:55.893465Z","iopub.status.busy":"2024-07-10T17:47:55.893092Z","iopub.status.idle":"2024-07-10T17:47:55.906165Z","shell.execute_reply":"2024-07-10T17:47:55.904750Z","shell.execute_reply.started":"2024-07-10T17:47:55.893435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, n_embd, context_size, n_heads, head_size):\n        super(MultiHeadedAttention, self).__init__()\n        \n        self.heads = nn.ModuleList([Head(n_embd=n_embd, head_size=head_size, context_size=context_size) for _ in range(n_heads)]) # BxTx (n_heads * head_size)\n        self.projection = nn.Linear(in_features=n_heads*head_size, out_features=n_embd) # ensures the output is going to be o shape BxTxn_embd (BxTxC) so that is can go through multiple attention block\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = torch.cat([head(x) for head in self.heads], dim=-1) # cat in the Channels dimension; output shape is BxTx (n_heads * head_size)\n        #print(f\"multihead output shape is {out.shape}\")\n        #return out\n\n        x = self.projection(x)\n        return  x","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:55:38.809805Z","iopub.status.busy":"2024-07-10T17:55:38.809296Z","iopub.status.idle":"2024-07-10T17:55:38.821918Z","shell.execute_reply":"2024-07-10T17:55:38.820738Z","shell.execute_reply.started":"2024-07-10T17:55:38.809767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, in_features):\n        super(FeedForward, self).__init__()\n\n        self.ffwrd_layer = nn.Sequential(\n            nn.Linear(in_features=in_features, out_features=in_features * 4), # scale by 4, according to the attention is all you need paper\n            nn.ReLU(),\n            nn.Linear(in_features=in_features * 4, out_features=in_features) # another projection layer\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.ffwrd_layer(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, n_heads, head_size, n_embd, context_size):\n        super(Block, self).__init__()\n        self.multiheaded_self_attetion = MultiHeadedAttention(n_embd=n_embd, context_size=context_size, n_heads=n_heads, head_size=head_size) # create a multiheaded attention block; returns shape BxTx (num_heads*head_size)\n        #self.ffwrd = FeedForward(in_features=n_heads*head_size) # returns shape BxTx (num_heads*head_size) --> this is only in case there is no projection layer inside of multiheaded attention\n        self.ffwrd = FeedForward(in_features=n_embd)\n\n        self.layer_norm1 = nn.LayerNorm(n_embd)\n        self.layer_norm2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # /// no residual connections ///\n        #x = self.multiheaded_self_attetion(x)\n        #x = self.ffwrd(x)\n\n        # /// with residual conections for better optimization ///\n        x = x + self.multiheaded_self_attetion(self.layer_norm1(x))\n        x = x + self.ffwrd(self.layer_norm2(x))\n\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, n_embd, context_size, vocab_size):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.context_size = context_size\n        self.n_embd = n_embd\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # each character from the vocab has n_embd values associated to it\n        self.positional_embedding_table = nn.Embedding(context_size, n_embd) # each character position in the context has n_embd values associated to it\n        \n        #self.linear1 = nn.Linear(in_features=n_embd, out_features=8*8)\n        #self.linear2 = nn.Linear(in_features=8*8, out_features=vocab_size)\n        #self.act_fn = nn.Tanh()\n        self.num_sa_heads = 16\n        #self.sa_head_size = 64\n        #self.sa_head_size = n_embd // self.num_sa_heads # this proportion is needed in case you are using multiple attention blocks so to keep proper dimensions, otherwhise you can set head_size to anything you want\n        self.sa_head_size = 64\n\n        #self.multiheadattention = MultiHeadedAttention(n_embd=n_embd, context_size=context_size, n_heads=self.num_sa_heads, head_size=self.sa_head_size)\n        #self.ffwrd = FeedForward(in_features=self.num_sa_heads*self.sa_head_size) # going to take in BxTx (sa_head_size * num_sa_heads) --> going to output the same shape\n        #self.sa_head = Head(n_embd=64, head_size=64, context_size=self.context_size)\n\n\n        self.attention_blocks = nn.Sequential(\n            Block(n_heads=self.num_sa_heads, head_size=self.sa_head_size, context_size=self.context_size, n_embd=self.n_embd), # takes in BxTxC, calculate logits of BxTx (num_heads * head_size), then project it as BxTxC\n            Block(n_heads=self.num_sa_heads, head_size=self.sa_head_size, context_size=self.context_size, n_embd=self.n_embd),\n            #Block(n_heads=self.num_sa_heads, head_size=self.sa_head_size, context_size=self.context_size, n_embd=self.n_embd)\n            nn.LayerNorm(n_embd)\n        )\n\n        #print(\"fs\")\n\n        #self.attention_blocks = Block(n_heads=self.num_sa_heads, head_size=self.sa_head_size, context_size=self.context_size, n_embd=self.n_embd)\n\n        #self.lm_head = nn.Linear(in_features=self.sa_head_size*self.num_sa_heads, out_features=vocab_size) # in case of attention_blocks with no projection layer\n        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        #print(f\"context_size is {self.context_size}\")\n        B, T = x.shape # batch_size and context_size\n        positions = torch.arange(start=0, end=T, step=1)\n        \n        pos_emb = self.positional_embedding_table(positions) # T x C --> in broadcasting, pytorch adds a batch dim=1\n        token_emb = self.token_embedding_table(x) # B x T x C\n        \n        x = token_emb + pos_emb # BxTxC\n        #x = self.lm_head(x) # BxTxVocab_size\n        #x = self.act_fn(self.linear1(x)) # BxTxVocab_size\n        #x = self.linear2(x) # BxTxVocab_size\n        \n        #x = self.multiheadattention(x) # BxTx (sa_head_size*num_sa_heads)\n        #x = self.ffwrd(x)\n        #self_attention = self.sa_head(x) # BxTxHead_size (BxTxC in this case, since head_size=n_embd)\n\n        x = self.attention_blocks(x) # returns logits of shape BxTx (self.sa_head_size * self.num_sa_heads) projected to BxTxC\n\n        x = self.lm_head(x) # BxTxvocab_size --> BxTxHead_size @ BxTxVocab_size return BxTxVocab_size\n\n        return x.view(B*T, self.vocab_size) # easier shape to work with the labels\n    \n    def generate(self, starting_idx: torch.Tensor, max_length: int, debug: bool) -> torch.Tensor:\n        full_text = decode([starting_idx.item()])\n\n        \n        context = starting_idx\n        \n        for _ in range(max_length):\n            context = context[:, -self.context_size:] # make sure the context is of size context_size\n            \n            if debug:\n                print(f\"predicting on context: {decode(context[0].tolist())}\")\n            \n            logits = self(context) # B*T x vocab_size --> 1*2 x vocab_size\n            logits = logits[-1, :].view(1, self.vocab_size) # only take the prediction for the last character\n            percents = torch.softmax(logits, dim=1) # 1*2xvocab_size\n            pred = torch.multinomial(percents, num_samples=1) \n            full_text += decode(pred.tolist()[0])\n            \n            #print(len(padded[0]))\n            context = torch.cat([context, pred], dim=1) # add to the context dimension instead of the batch dim\n            \n        return full_text","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:56:44.297656Z","iopub.status.busy":"2024-07-10T17:56:44.297241Z","iopub.status.idle":"2024-07-10T17:56:44.310380Z","shell.execute_reply":"2024-07-10T17:56:44.309165Z","shell.execute_reply.started":"2024-07-10T17:56:44.297625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class model_generator:\n    def __init__(self, model: object, max_length: int, num_samples: int, vocab_size: int):\n        self.model = model\n        self.max_length = max_length\n        self.num_samples = num_samples\n        self.vocab_size = vocab_size\n        \n        self.last_output = \"\"\n        \n        self.params_dict = {\n            \"model\": model,\n            \"max_length\": max_length,\n            \"num_samples\": num_samples,\n            \"previous_outputs\": []\n        }\n    \n    @torch.no_grad\n    def generate(self, starting_char: str = None, clear_outputs: bool = True, debug: bool = False):\n        self.model.eval()\n        \n        if clear_outputs:\n            self.clear_ouptuts()\n            \n        if starting_char is None:\n            starting_char = decode([torch.randint(0, vocab_size, (1,)).item()])\n            \n        for _ in range(self.num_samples):\n            starting_idx = torch.tensor(encode(starting_char), dtype=torch.long).view(1, 1)\n            output = model.generate(starting_idx=starting_idx, max_length=self.max_length, debug=debug)\n            self.params_dict[\"previous_outputs\"].append(output)\n            self.last_output = output\n    \n    def update_params(self, model: object = None, max_length: int = None, num_samples: int = None, clear_outputs: bool = None):\n        if clear_outputs:\n            self.clear_outputs()\n            \n        updated_dict = {\n            \"model\": model,\n            \"max_length\": max_length,\n            \"num_samples\": num_samples\n        }\n        \n        for attribute, value in updated_dict.items():\n            if value is not None:\n                self.params_dict[attribute] = value\n                setattr(self, attribute, value)\n    \n    def clear_ouptuts(self):\n        self.params_dict[\"previous_outputs\"] = []\n        self.last_output = \"\"\n        \n    def print_outputs(self, last: bool = None):\n        if last:\n            print(self.last_output)\n        else:\n            for output in self.params_dict[\"previous_outputs\"]:\n                print(f\"{output}\\n\\n\")\n            ","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:56:44.435360Z","iopub.status.busy":"2024-07-10T17:56:44.434253Z","iopub.status.idle":"2024-07-10T17:56:44.447754Z","shell.execute_reply":"2024-07-10T17:56:44.446347Z","shell.execute_reply.started":"2024-07-10T17:56:44.435324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_embd = 1024\nvocab_size = len(vocab)\nmodel = MLP(n_embd=n_embd, context_size=context_size, vocab_size=vocab_size)\nmlp_generator = model_generator(model=model, max_length=32, num_samples=1, vocab_size=vocab_size)\n#print(model)\nmlp_generator.generate(starting_char=\".\", debug=False)\nmlp_generator.print_outputs()","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:56:46.138956Z","iopub.status.busy":"2024-07-10T17:56:46.138023Z","iopub.status.idle":"2024-07-10T17:56:46.396397Z","shell.execute_reply":"2024-07-10T17:56:46.395260Z","shell.execute_reply.started":"2024-07-10T17:56:46.138919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mlp_generator.clear_ouptuts()\nmlp_generator.update_params(max_length=100, num_samples=1)\nmlp_generator.generate()\nmlp_generator.print_outputs()","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:56:46.596279Z","iopub.status.busy":"2024-07-10T17:56:46.595903Z","iopub.status.idle":"2024-07-10T17:56:46.755175Z","shell.execute_reply":"2024-07-10T17:56:46.753980Z","shell.execute_reply.started":"2024-07-10T17:56:46.596250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ntest_loss_fn = nn.CrossEntropyLoss()\nbatch_sample_inputs, batch_sample_labels = next(iter(train_dataloader))\nwith torch.inference_mode():\n    #print(batch_sample_inputs)\n    #print(batch_sample_labels)\n    logits = model(batch_sample_inputs)\n    labels = batch_sample_labels.view(-1) # turns the label shape into a B*T\n    #print(logits) # 4 batches of 8 characters each, the model is trying to predict the next sequence\n    #print(labels)\n    loss = test_loss_fn(logits, batch_sample_labels.view(-1))\n    print(loss)","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:56:47.108852Z","iopub.status.busy":"2024-07-10T17:56:47.108035Z","iopub.status.idle":"2024-07-10T17:56:47.126700Z","shell.execute_reply":"2024-07-10T17:56:47.125545Z","shell.execute_reply.started":"2024-07-10T17:56:47.108813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, loss_fn, optimizer, epochs):\n    model.train()\n    \n    for epoch in range(epochs):\n        for batch, (X, y) in tqdm(enumerate(dataloader)):\n            logits = model(X) # shape of B*T x vocab_size\n            labels = y.view(-1) # shape of B*T --> each character has it's own prediction\n            loss = loss_fn(logits, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            if batch % 100 == 0:\n                print(f\"loss for batch {batch} --> {loss} at epoch {epoch}\")","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:56:47.644885Z","iopub.status.busy":"2024-07-10T17:56:47.644490Z","iopub.status.idle":"2024-07-10T17:56:47.652221Z","shell.execute_reply":"2024-07-10T17:56:47.650965Z","shell.execute_reply.started":"2024-07-10T17:56:47.644852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:56:48.276778Z","iopub.status.busy":"2024-07-10T17:56:48.275952Z","iopub.status.idle":"2024-07-10T17:56:48.282641Z","shell.execute_reply":"2024-07-10T17:56:48.281354Z","shell.execute_reply.started":"2024-07-10T17:56:48.276740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(model=model, dataloader=train_dataloader, loss_fn=loss_fn, optimizer=optimizer, epochs=15)","metadata":{"execution":{"iopub.execute_input":"2024-07-10T17:56:50.716605Z","iopub.status.busy":"2024-07-10T17:56:50.716165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp_generator.update_params(max_length=1000, num_samples=5)\nmlp_generator.generate()\nmlp_generator.print_outputs() # from those three outputs, print the last one","metadata":{"execution":{"iopub.status.busy":"2024-07-10T20:01:54.095464Z","iopub.execute_input":"2024-07-10T20:01:54.095983Z","iopub.status.idle":"2024-07-10T20:01:54.525171Z","shell.execute_reply.started":"2024-07-10T20:01:54.095948Z","shell.execute_reply":"2024-07-10T20:01:54.523540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}