{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94113c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:01.069441Z",
     "iopub.status.busy": "2024-07-10T20:05:01.068992Z",
     "iopub.status.idle": "2024-07-10T20:05:04.740070Z",
     "shell.execute_reply": "2024-07-10T20:05:04.738843Z"
    },
    "papermill": {
     "duration": 3.684319,
     "end_time": "2024-07-10T20:05:04.742608",
     "exception": false,
     "start_time": "2024-07-10T20:05:01.058289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2aab56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:04.763584Z",
     "iopub.status.busy": "2024-07-10T20:05:04.762810Z",
     "iopub.status.idle": "2024-07-10T20:05:04.855295Z",
     "shell.execute_reply": "2024-07-10T20:05:04.853901Z"
    },
    "papermill": {
     "duration": 0.105362,
     "end_time": "2024-07-10T20:05:04.857170",
     "exception": false,
     "start_time": "2024-07-10T20:05:04.751808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default device set to cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "generator = torch.Generator(device=device)\n",
    "torch.set_default_device(device)\n",
    "print(f\"default device set to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d0c1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:04.882498Z",
     "iopub.status.busy": "2024-07-10T20:05:04.881811Z",
     "iopub.status.idle": "2024-07-10T20:05:04.912948Z",
     "shell.execute_reply": "2024-07-10T20:05:04.911755Z"
    },
    "papermill": {
     "duration": 0.046929,
     "end_time": "2024-07-10T20:05:04.915592",
     "exception": false,
     "start_time": "2024-07-10T20:05:04.868663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/shakespeare/input.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d56306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:04.935966Z",
     "iopub.status.busy": "2024-07-10T20:05:04.935596Z",
     "iopub.status.idle": "2024-07-10T20:05:04.961624Z",
     "shell.execute_reply": "2024-07-10T20:05:04.960696Z"
    },
    "papermill": {
     "duration": 0.039619,
     "end_time": "2024-07-10T20:05:04.964449",
     "exception": false,
     "start_time": "2024-07-10T20:05:04.924830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4775d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:04.992144Z",
     "iopub.status.busy": "2024-07-10T20:05:04.991764Z",
     "iopub.status.idle": "2024-07-10T20:05:04.998339Z",
     "shell.execute_reply": "2024-07-10T20:05:04.997175Z"
    },
    "papermill": {
     "duration": 0.022974,
     "end_time": "2024-07-10T20:05:05.000738",
     "exception": false,
     "start_time": "2024-07-10T20:05:04.977764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "stoi = {c: v for v, c in enumerate(vocab)}\n",
    "itos = {v: c for c, v in stoi.items()}\n",
    "\n",
    "print(stoi[\"h\"])\n",
    "print(itos[46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bc6782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:05.021754Z",
     "iopub.status.busy": "2024-07-10T20:05:05.020930Z",
     "iopub.status.idle": "2024-07-10T20:05:05.027014Z",
     "shell.execute_reply": "2024-07-10T20:05:05.026074Z"
    },
    "papermill": {
     "duration": 0.019994,
     "end_time": "2024-07-10T20:05:05.029724",
     "exception": false,
     "start_time": "2024-07-10T20:05:05.009730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 46, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59, 12] hello how are you?\n"
     ]
    }
   ],
   "source": [
    "encode = lambda e: [stoi[ch] for ch in e]\n",
    "decode = lambda d: \"\".join([itos[idx] for idx in d])\n",
    "\n",
    "encoded = encode(\"hello how are you?\")\n",
    "decoded = decode(encoded)\n",
    "\n",
    "print(encoded, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73aee4f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:05.050262Z",
     "iopub.status.busy": "2024-07-10T20:05:05.049721Z",
     "iopub.status.idle": "2024-07-10T20:05:05.399464Z",
     "shell.execute_reply": "2024-07-10T20:05:05.398416Z"
    },
    "papermill": {
     "duration": 0.363873,
     "end_time": "2024-07-10T20:05:05.402101",
     "exception": false,
     "start_time": "2024-07-10T20:05:05.038228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3072, 8960, 7424, 6656, 1024, 5632, 2304, 2560, 4608, 7680, 1280, 5888,\n",
      "        7168,  512, 9728, 9216,  256,    0, 8704, 3584, 1792, 6144, 3328,  768,\n",
      "        4864, 8192, 5376, 1536, 5120, 2048, 4096, 2816, 4352, 6912, 3840, 6400,\n",
      "        9472, 7936, 8448], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "context_size = 256\n",
    "random_idx_tensor = torch.randperm(10000//context_size) * context_size\n",
    "print(random_idx_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b28930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:05.423086Z",
     "iopub.status.busy": "2024-07-10T20:05:05.422703Z",
     "iopub.status.idle": "2024-07-10T20:05:05.429464Z",
     "shell.execute_reply": "2024-07-10T20:05:05.428357Z"
    },
    "papermill": {
     "duration": 0.020015,
     "end_time": "2024-07-10T20:05:05.431665",
     "exception": false,
     "start_time": "2024-07-10T20:05:05.411650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset(data):\n",
    "    random_idx_tensor = torch.randperm((len(data)-context_size)//context_size) * context_size\n",
    "    inputs = torch.stack([data[idx:idx+context_size] for idx in random_idx_tensor])\n",
    "    labels = torch.stack([data[idx+1:idx+context_size+1] for idx in random_idx_tensor])\n",
    "    \n",
    "    return TensorDataset(inputs.to(torch.long), labels.to(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7508ff73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:05.451484Z",
     "iopub.status.busy": "2024-07-10T20:05:05.451113Z",
     "iopub.status.idle": "2024-07-10T20:05:06.681020Z",
     "shell.execute_reply": "2024-07-10T20:05:06.680019Z"
    },
    "papermill": {
     "duration": 1.242369,
     "end_time": "2024-07-10T20:05:06.683378",
     "exception": false,
     "start_time": "2024-07-10T20:05:05.441009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([53, 42,  1, 58, 46, 39, 58,  1, 50, 43, 58,  1, 58, 46, 47, 57,  1, 40,\n",
      "        50, 53, 53, 42,  1, 44, 56, 53, 51,  1, 46, 43, 52, 41, 43,  2,  0, 25,\n",
      "        53, 56, 43,  1, 42, 47, 56, 43, 44, 59, 50,  1, 46, 39, 54,  1, 40, 43,\n",
      "        58, 47, 42, 43,  1, 58, 46, 39, 58,  1, 46, 39, 58, 43, 42,  1, 61, 56,\n",
      "        43, 58, 41, 46,  6,  0, 32, 46, 39, 58,  1, 51, 39, 49, 43, 57,  1, 59,\n",
      "        57,  1, 61, 56, 43, 58, 41, 46, 43, 42,  1, 40, 63,  1, 58, 46, 43,  1,\n",
      "        42, 43, 39, 58, 46,  1, 53, 44,  1, 58, 46, 43, 43,  6,  0, 32, 46, 39,\n",
      "        52,  1, 21,  1, 41, 39, 52,  1, 61, 47, 57, 46,  1, 58, 53,  1, 39, 42,\n",
      "        42, 43, 56, 57,  6,  1, 57, 54, 47, 42, 43, 56, 57,  6,  1, 58, 53, 39,\n",
      "        42, 57,  6,  0, 27, 56,  1, 39, 52, 63,  1, 41, 56, 43, 43, 54, 47, 52,\n",
      "        45,  1, 60, 43, 52, 53, 51,  5, 42,  1, 58, 46, 47, 52, 45,  1, 58, 46,\n",
      "        39, 58,  1, 50, 47, 60, 43, 57,  2,  0, 21, 44,  1, 43, 60, 43, 56,  1,\n",
      "        46, 43,  1, 46, 39, 60, 43,  1, 41, 46, 47, 50, 42,  6,  1, 39, 40, 53,\n",
      "        56, 58, 47, 60, 43,  1, 40, 43,  1, 47, 58,  6,  0, 28, 56, 53, 42, 47,\n",
      "        45, 47, 53, 59], device='cuda:0')\n",
      "tensor([42,  1, 58, 46, 39, 58,  1, 50, 43, 58,  1, 58, 46, 47, 57,  1, 40, 50,\n",
      "        53, 53, 42,  1, 44, 56, 53, 51,  1, 46, 43, 52, 41, 43,  2,  0, 25, 53,\n",
      "        56, 43,  1, 42, 47, 56, 43, 44, 59, 50,  1, 46, 39, 54,  1, 40, 43, 58,\n",
      "        47, 42, 43,  1, 58, 46, 39, 58,  1, 46, 39, 58, 43, 42,  1, 61, 56, 43,\n",
      "        58, 41, 46,  6,  0, 32, 46, 39, 58,  1, 51, 39, 49, 43, 57,  1, 59, 57,\n",
      "         1, 61, 56, 43, 58, 41, 46, 43, 42,  1, 40, 63,  1, 58, 46, 43,  1, 42,\n",
      "        43, 39, 58, 46,  1, 53, 44,  1, 58, 46, 43, 43,  6,  0, 32, 46, 39, 52,\n",
      "         1, 21,  1, 41, 39, 52,  1, 61, 47, 57, 46,  1, 58, 53,  1, 39, 42, 42,\n",
      "        43, 56, 57,  6,  1, 57, 54, 47, 42, 43, 56, 57,  6,  1, 58, 53, 39, 42,\n",
      "        57,  6,  0, 27, 56,  1, 39, 52, 63,  1, 41, 56, 43, 43, 54, 47, 52, 45,\n",
      "         1, 60, 43, 52, 53, 51,  5, 42,  1, 58, 46, 47, 52, 45,  1, 58, 46, 39,\n",
      "        58,  1, 50, 47, 60, 43, 57,  2,  0, 21, 44,  1, 43, 60, 43, 56,  1, 46,\n",
      "        43,  1, 46, 39, 60, 43,  1, 41, 46, 47, 50, 42,  6,  1, 39, 40, 53, 56,\n",
      "        58, 47, 60, 43,  1, 40, 43,  1, 47, 58,  6,  0, 28, 56, 53, 42, 47, 45,\n",
      "        47, 53, 59, 57], device='cuda:0')\n",
      "dataset length --> 4356 (1115136 characters), that is, about the length of text 1115394 - context_size --> 1115138\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "dataset = make_dataset(data=data)\n",
    "sample_input = dataset[0][0]\n",
    "sample_label = dataset[0][1]\n",
    "\n",
    "print(sample_input)\n",
    "print(sample_label)\n",
    "\n",
    "print(f\"dataset length --> {len(dataset)} ({len(dataset) * context_size} characters), that is, about the length of text {len(text)} - context_size --> {len(text)-context_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee48261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.702482Z",
     "iopub.status.busy": "2024-07-10T20:05:06.702163Z",
     "iopub.status.idle": "2024-07-10T20:05:06.708586Z",
     "shell.execute_reply": "2024-07-10T20:05:06.707805Z"
    },
    "papermill": {
     "duration": 0.018653,
     "end_time": "2024-07-10T20:05:06.710833",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.692180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)-train_split)\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset=dataset, lengths=[train_split, test_split], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5695f65a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.729953Z",
     "iopub.status.busy": "2024-07-10T20:05:06.729544Z",
     "iopub.status.idle": "2024-07-10T20:05:06.735165Z",
     "shell.execute_reply": "2024-07-10T20:05:06.734083Z"
    },
    "papermill": {
     "duration": 0.017855,
     "end_time": "2024-07-10T20:05:06.737208",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.719353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777ec292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.756219Z",
     "iopub.status.busy": "2024-07-10T20:05:06.755780Z",
     "iopub.status.idle": "2024-07-10T20:05:06.765512Z",
     "shell.execute_reply": "2024-07-10T20:05:06.764571Z"
    },
    "papermill": {
     "duration": 0.0214,
     "end_time": "2024-07-10T20:05:06.767554",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.746154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, context_size):\n",
    "        super(Head, self).__init__()\n",
    "        \n",
    "        self.Q = nn.Linear(in_features=n_embd, out_features=head_size) # takes in BxTxC and return BxTxHead_size\n",
    "        self.K = nn.Linear(in_features=n_embd, out_features=head_size)\n",
    "        self.V = nn.Linear(in_features=n_embd, out_features=head_size)\n",
    "        \n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(size=(context_size, context_size))))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape # batch_size by context_size by n_embd\n",
    "        q = self.Q(x) # BxTxHead_size\n",
    "        k = self.K(x) # BxTxHead_size\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5) # BxTxHead_size @ BxHead_sizexT --> BxTxT then divided by the square root of n_embd\n",
    "        \n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # :T and :T is needed in case context is smaller than context_size\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "        v = self.V(x) # BxTxHead_size\n",
    "        #print(f\"wei shape is {wei.shape}\")\n",
    "        output = wei @ v # BxTxT @ BxTxHead_size --> BxTxHead_size\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8e9ae05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.787378Z",
     "iopub.status.busy": "2024-07-10T20:05:06.786534Z",
     "iopub.status.idle": "2024-07-10T20:05:06.794906Z",
     "shell.execute_reply": "2024-07-10T20:05:06.793742Z"
    },
    "papermill": {
     "duration": 0.020959,
     "end_time": "2024-07-10T20:05:06.797101",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.776142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_embd, context_size, n_heads, head_size):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(n_embd=n_embd, head_size=head_size, context_size=context_size) for _ in range(n_heads)]) # BxTx (n_heads * head_size)\n",
    "        self.projection = nn.Linear(in_features=n_heads*head_size, out_features=n_embd) # ensures the output is going to be o shape BxTxn_embd (BxTxC) so that is can go through multiple attention block\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1) # cat in the Channels dimension; output shape is BxTx (n_heads * head_size)\n",
    "        #print(f\"multihead output shape is {out.shape}\")\n",
    "        #return out\n",
    "\n",
    "        x = self.projection(x)\n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c625f9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.817708Z",
     "iopub.status.busy": "2024-07-10T20:05:06.817299Z",
     "iopub.status.idle": "2024-07-10T20:05:06.824392Z",
     "shell.execute_reply": "2024-07-10T20:05:06.823444Z"
    },
    "papermill": {
     "duration": 0.020049,
     "end_time": "2024-07-10T20:05:06.826734",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.806685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.ffwrd_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=in_features * 4), # scale by 4, according to the attention is all you need paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=in_features * 4, out_features=in_features) # another projection layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ffwrd_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5db9f76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.847755Z",
     "iopub.status.busy": "2024-07-10T20:05:06.847376Z",
     "iopub.status.idle": "2024-07-10T20:05:06.855616Z",
     "shell.execute_reply": "2024-07-10T20:05:06.854579Z"
    },
    "papermill": {
     "duration": 0.021399,
     "end_time": "2024-07-10T20:05:06.857803",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.836404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, n_embd, context_size):\n",
    "        super(Block, self).__init__()\n",
    "        self.multiheaded_self_attetion = MultiHeadedAttention(n_embd=n_embd, context_size=context_size, n_heads=n_heads, head_size=head_size) # create a multiheaded attention block; returns shape BxTx (num_heads*head_size)\n",
    "        #self.ffwrd = FeedForward(in_features=n_heads*head_size) # returns shape BxTx (num_heads*head_size) --> this is only in case there is no projection layer inside of multiheaded attention\n",
    "        self.ffwrd = FeedForward(in_features=n_embd)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # /// no residual connections ///\n",
    "        #x = self.multiheaded_self_attetion(x)\n",
    "        #x = self.ffwrd(x)\n",
    "\n",
    "        # /// with residual conections for better optimization ///\n",
    "        x = x + self.multiheaded_self_attetion(self.layer_norm1(x))\n",
    "        x = x + self.ffwrd(self.layer_norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b7b56d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.878031Z",
     "iopub.status.busy": "2024-07-10T20:05:06.877642Z",
     "iopub.status.idle": "2024-07-10T20:05:06.894041Z",
     "shell.execute_reply": "2024-07-10T20:05:06.892907Z"
    },
    "papermill": {
     "duration": 0.02955,
     "end_time": "2024-07-10T20:05:06.896372",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.866822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd, context_size, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.n_embd = n_embd\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # each character from the vocab has n_embd values associated to it\n",
    "        self.positional_embedding_table = nn.Embedding(context_size, n_embd) # each character position in the context has n_embd values associated to it\n",
    "        \n",
    "        #self.linear1 = nn.Linear(in_features=n_embd, out_features=8*8)\n",
    "        #self.linear2 = nn.Linear(in_features=8*8, out_features=vocab_size)\n",
    "        #self.act_fn = nn.Tanh()\n",
    "        self.num_sa_heads = 16\n",
    "        #self.sa_head_size = 64\n",
    "        #self.sa_head_size = n_embd // self.num_sa_heads # this proportion is needed in case you are using multiple attention blocks so to keep proper dimensions, otherwhise you can set head_size to anything you want\n",
    "        self.sa_head_size = 64\n",
    "\n",
    "        #self.multiheadattention = MultiHeadedAttention(n_embd=n_embd, context_size=context_size, n_heads=self.num_sa_heads, head_size=self.sa_head_size)\n",
    "        #self.ffwrd = FeedForward(in_features=self.num_sa_heads*self.sa_head_size) # going to take in BxTx (sa_head_size * num_sa_heads) --> going to output the same shape\n",
    "        #self.sa_head = Head(n_embd=64, head_size=64, context_size=self.context_size)\n",
    "\n",
    "\n",
    "        self.attention_blocks = nn.Sequential(\n",
    "            Block(n_heads=self.num_sa_heads, head_size=self.sa_head_size, context_size=self.context_size, n_embd=self.n_embd), # takes in BxTxC, calculate logits of BxTx (num_heads * head_size), then project it as BxTxC\n",
    "            Block(n_heads=self.num_sa_heads, head_size=self.sa_head_size, context_size=self.context_size, n_embd=self.n_embd),\n",
    "            #Block(n_heads=self.num_sa_heads, head_size=self.sa_head_size, context_size=self.context_size, n_embd=self.n_embd)\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "\n",
    "        #print(\"fs\")\n",
    "\n",
    "        #self.attention_blocks = Block(n_heads=self.num_sa_heads, head_size=self.sa_head_size, context_size=self.context_size, n_embd=self.n_embd)\n",
    "\n",
    "        #self.lm_head = nn.Linear(in_features=self.sa_head_size*self.num_sa_heads, out_features=vocab_size) # in case of attention_blocks with no projection layer\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(f\"context_size is {self.context_size}\")\n",
    "        B, T = x.shape # batch_size and context_size\n",
    "        positions = torch.arange(start=0, end=T, step=1)\n",
    "        \n",
    "        pos_emb = self.positional_embedding_table(positions) # T x C --> in broadcasting, pytorch adds a batch dim=1\n",
    "        token_emb = self.token_embedding_table(x) # B x T x C\n",
    "        \n",
    "        x = token_emb + pos_emb # BxTxC\n",
    "        #x = self.lm_head(x) # BxTxVocab_size\n",
    "        #x = self.act_fn(self.linear1(x)) # BxTxVocab_size\n",
    "        #x = self.linear2(x) # BxTxVocab_size\n",
    "        \n",
    "        #x = self.multiheadattention(x) # BxTx (sa_head_size*num_sa_heads)\n",
    "        #x = self.ffwrd(x)\n",
    "        #self_attention = self.sa_head(x) # BxTxHead_size (BxTxC in this case, since head_size=n_embd)\n",
    "\n",
    "        x = self.attention_blocks(x) # returns logits of shape BxTx (self.sa_head_size * self.num_sa_heads) projected to BxTxC\n",
    "\n",
    "        x = self.lm_head(x) # BxTxvocab_size --> BxTxHead_size @ BxTxVocab_size return BxTxVocab_size\n",
    "\n",
    "        return x.view(B*T, self.vocab_size) # easier shape to work with the labels\n",
    "    \n",
    "    def generate(self, starting_idx: torch.Tensor, max_length: int, debug: bool) -> torch.Tensor:\n",
    "        full_text = decode([starting_idx.item()])\n",
    "\n",
    "        \n",
    "        context = starting_idx\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            context = context[:, -self.context_size:] # make sure the context is of size context_size\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"predicting on context: {decode(context[0].tolist())}\")\n",
    "            \n",
    "            logits = self(context) # B*T x vocab_size --> 1*2 x vocab_size\n",
    "            logits = logits[-1, :].view(1, self.vocab_size) # only take the prediction for the last character\n",
    "            percents = torch.softmax(logits, dim=1) # 1*2xvocab_size\n",
    "            pred = torch.multinomial(percents, num_samples=1) \n",
    "            full_text += decode(pred.tolist()[0])\n",
    "            \n",
    "            #print(len(padded[0]))\n",
    "            context = torch.cat([context, pred], dim=1) # add to the context dimension instead of the batch dim\n",
    "            \n",
    "        return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9680eb0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.917391Z",
     "iopub.status.busy": "2024-07-10T20:05:06.916522Z",
     "iopub.status.idle": "2024-07-10T20:05:06.931203Z",
     "shell.execute_reply": "2024-07-10T20:05:06.930171Z"
    },
    "papermill": {
     "duration": 0.027491,
     "end_time": "2024-07-10T20:05:06.933348",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.905857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class model_generator:\n",
    "    def __init__(self, model: object, max_length: int, num_samples: int, vocab_size: int):\n",
    "        self.model = model\n",
    "        self.max_length = max_length\n",
    "        self.num_samples = num_samples\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.last_output = \"\"\n",
    "        \n",
    "        self.params_dict = {\n",
    "            \"model\": model,\n",
    "            \"max_length\": max_length,\n",
    "            \"num_samples\": num_samples,\n",
    "            \"previous_outputs\": []\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def generate(self, starting_char: str = None, clear_outputs: bool = True, debug: bool = False):\n",
    "        self.model.eval()\n",
    "        \n",
    "        if clear_outputs:\n",
    "            self.clear_ouptuts()\n",
    "            \n",
    "        if starting_char is None:\n",
    "            starting_char = decode([torch.randint(0, vocab_size, (1,)).item()])\n",
    "            \n",
    "        for _ in range(self.num_samples):\n",
    "            starting_idx = torch.tensor(encode(starting_char), dtype=torch.long).view(1, 1)\n",
    "            output = model.generate(starting_idx=starting_idx, max_length=self.max_length, debug=debug)\n",
    "            self.params_dict[\"previous_outputs\"].append(output)\n",
    "            self.last_output = output\n",
    "    \n",
    "    def update_params(self, model: object = None, max_length: int = None, num_samples: int = None, clear_outputs: bool = None):\n",
    "        if clear_outputs:\n",
    "            self.clear_outputs()\n",
    "            \n",
    "        updated_dict = {\n",
    "            \"model\": model,\n",
    "            \"max_length\": max_length,\n",
    "            \"num_samples\": num_samples\n",
    "        }\n",
    "        \n",
    "        for attribute, value in updated_dict.items():\n",
    "            if value is not None:\n",
    "                self.params_dict[attribute] = value\n",
    "                setattr(self, attribute, value)\n",
    "    \n",
    "    def clear_ouptuts(self):\n",
    "        self.params_dict[\"previous_outputs\"] = []\n",
    "        self.last_output = \"\"\n",
    "        \n",
    "    def print_outputs(self, last: bool = None):\n",
    "        if last:\n",
    "            print(self.last_output)\n",
    "        else:\n",
    "            for output in self.params_dict[\"previous_outputs\"]:\n",
    "                print(f\"{output}\\n\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bda5c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:06.954677Z",
     "iopub.status.busy": "2024-07-10T20:05:06.953921Z",
     "iopub.status.idle": "2024-07-10T20:05:08.055480Z",
     "shell.execute_reply": "2024-07-10T20:05:08.054462Z"
    },
    "papermill": {
     "duration": 1.114627,
     "end_time": "2024-07-10T20:05:08.057660",
     "exception": false,
     "start_time": "2024-07-10T20:05:06.943033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".kpco.ECVf\n",
      "YHpshY,\n",
      "NBzoVcLgs;SxNb\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_embd = 1024\n",
    "vocab_size = len(vocab)\n",
    "model = MLP(n_embd=n_embd, context_size=context_size, vocab_size=vocab_size)\n",
    "mlp_generator = model_generator(model=model, max_length=32, num_samples=1, vocab_size=vocab_size)\n",
    "#print(model)\n",
    "mlp_generator.generate(starting_char=\".\", debug=False)\n",
    "mlp_generator.print_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff065c1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:08.077043Z",
     "iopub.status.busy": "2024-07-10T20:05:08.076640Z",
     "iopub.status.idle": "2024-07-10T20:05:09.573162Z",
     "shell.execute_reply": "2024-07-10T20:05:09.572007Z"
    },
    "papermill": {
     "duration": 1.509443,
     "end_time": "2024-07-10T20:05:09.575912",
     "exception": false,
     "start_time": "2024-07-10T20:05:08.066469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fI-qzXUhO$:QkKYFcmjMo;g\n",
      ";hhUTg&BsKO&.As&\n",
      ",Uy.$bj OsoArwYjAbrRxNbolVSLhLYHVvL-e!$Y&HjSYyi&RvJaqelAMoPh\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mlp_generator.clear_ouptuts()\n",
    "mlp_generator.update_params(max_length=100, num_samples=1)\n",
    "mlp_generator.generate()\n",
    "mlp_generator.print_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "796843dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:09.596869Z",
     "iopub.status.busy": "2024-07-10T20:05:09.596473Z",
     "iopub.status.idle": "2024-07-10T20:05:09.835482Z",
     "shell.execute_reply": "2024-07-10T20:05:09.834315Z"
    },
    "papermill": {
     "duration": 0.252536,
     "end_time": "2024-07-10T20:05:09.838372",
     "exception": false,
     "start_time": "2024-07-10T20:05:09.585836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3491, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss_fn = nn.CrossEntropyLoss()\n",
    "batch_sample_inputs, batch_sample_labels = next(iter(train_dataloader))\n",
    "with torch.inference_mode():\n",
    "    #print(batch_sample_inputs)\n",
    "    #print(batch_sample_labels)\n",
    "    logits = model(batch_sample_inputs)\n",
    "    labels = batch_sample_labels.view(-1) # turns the label shape into a B*T\n",
    "    #print(logits) # 4 batches of 8 characters each, the model is trying to predict the next sequence\n",
    "    #print(labels)\n",
    "    loss = test_loss_fn(logits, batch_sample_labels.view(-1))\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d87d72e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:09.859590Z",
     "iopub.status.busy": "2024-07-10T20:05:09.859171Z",
     "iopub.status.idle": "2024-07-10T20:05:09.866454Z",
     "shell.execute_reply": "2024-07-10T20:05:09.865430Z"
    },
    "papermill": {
     "duration": 0.020218,
     "end_time": "2024-07-10T20:05:09.868686",
     "exception": false,
     "start_time": "2024-07-10T20:05:09.848468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch, (X, y) in tqdm(enumerate(dataloader)):\n",
    "            logits = model(X) # shape of B*T x vocab_size\n",
    "            labels = y.view(-1) # shape of B*T --> each character has it's own prediction\n",
    "            loss = loss_fn(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                print(f\"loss for batch {batch} --> {loss} at epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e89c645a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:09.889131Z",
     "iopub.status.busy": "2024-07-10T20:05:09.888700Z",
     "iopub.status.idle": "2024-07-10T20:05:11.178596Z",
     "shell.execute_reply": "2024-07-10T20:05:11.177795Z"
    },
    "papermill": {
     "duration": 1.303027,
     "end_time": "2024-07-10T20:05:11.181177",
     "exception": false,
     "start_time": "2024-07-10T20:05:09.878150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0555061b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:05:11.200655Z",
     "iopub.status.busy": "2024-07-10T20:05:11.200154Z",
     "iopub.status.idle": "2024-07-10T20:15:31.269391Z",
     "shell.execute_reply": "2024-07-10T20:15:31.268132Z"
    },
    "papermill": {
     "duration": 620.08179,
     "end_time": "2024-07-10T20:15:31.271808",
     "exception": false,
     "start_time": "2024-07-10T20:05:11.190018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 4.356245994567871 at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:38,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 2.461775064468384 at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:38,  2.65it/s]\n",
      "1it [00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 2.458270788192749 at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:39,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 2.343658685684204 at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:40,  2.57it/s]\n",
      "1it [00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 2.344531536102295 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.9427663087844849 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.47it/s]\n",
      "1it [00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.9983443021774292 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.7620501518249512 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.47it/s]\n",
      "1it [00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.7475244998931885 at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.6774563789367676 at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.48it/s]\n",
      "1it [00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.5802319049835205 at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.5720903873443604 at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.47it/s]\n",
      "1it [00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.4834562540054321 at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.4508671760559082 at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.48it/s]\n",
      "1it [00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.40569269657135 at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.3956351280212402 at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.48it/s]\n",
      "1it [00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.300905466079712 at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.319588541984558 at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.48it/s]\n",
      "1it [00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.24492347240448 at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.2452820539474487 at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.47it/s]\n",
      "1it [00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.1223393678665161 at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.1944148540496826 at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.48it/s]\n",
      "1it [00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 1.1027522087097168 at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.1709363460540771 at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.47it/s]\n",
      "1it [00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 0.988415002822876 at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 1.061604619026184 at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.48it/s]\n",
      "1it [00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 0.86592036485672 at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 0.954278290271759 at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.48it/s]\n",
      "1it [00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 0 --> 0.7514156103134155 at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:41,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for batch 100 --> 0.8967639803886414 at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:41,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "train_model(model=model, dataloader=train_dataloader, loss_fn=loss_fn, optimizer=optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21790189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:15:31.546193Z",
     "iopub.status.busy": "2024-07-10T20:15:31.545822Z",
     "iopub.status.idle": "2024-07-10T20:16:46.894938Z",
     "shell.execute_reply": "2024-07-10T20:16:46.893737Z"
    },
    "papermill": {
     "duration": 75.633373,
     "end_time": "2024-07-10T20:16:47.042621",
     "exception": false,
     "start_time": "2024-07-10T20:15:31.409248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward, recover me  his at\n",
      "is awas with power i' the morn.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "ARadon thy jesty, too lasty, ay.\n",
      "\n",
      "KING EDWARD IV:\n",
      "And, delight the should not, show I may sted.\n",
      "\n",
      "GLOUCESTER:\n",
      "Bags' shower:\n",
      "Consumere to this offerer pression tears?\n",
      "Marshall doth great Rard of Frons, for Senses my souls.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Or well, bear stay bewith sick, set means for man.\n",
      "\n",
      "WARWILA:\n",
      "MERS OF ARGH:\n",
      "And of Warwick conclusineasy.\n",
      "\n",
      "WARWICK:\n",
      "I will be say that want was scorn like aways;\n",
      "And 'gone of Romeo sly,\n",
      "'Tis forgot the run of dead, whom so.\n",
      "\n",
      "All: now take goes true, where at makes him?\n",
      "\n",
      "CORIOLANUS:\n",
      "Af O, for\n",
      "Away, whose I make make ofter, belie,\n",
      "Fasy. What were men your raw nch'd ver grievest thes?\n",
      "\n",
      "MENEN:\n",
      "Your high is least your will, go wises have;\n",
      "He wise, let weighbour ick-to his true,--\n",
      "And obscure at to be for heart.\n",
      "\n",
      "RICHARIT:\n",
      "But with hard, mother, dispossees have!\n",
      "\n",
      "PETRUCHIRGS:\n",
      "It choose seen that ans swear too, who ker.\n",
      "\n",
      "DUCHERESS Of GY:\n",
      "And thour hazer' lick with slip your is ly.\n",
      "\n",
      "PR\n",
      "\n",
      "\n",
      "f it.'\n",
      "\n",
      "First Senator:\n",
      "Our army's of your press' nument,\n",
      "I know your deserves Mere your kneess\n",
      "To tears o' your grand condemned, I would mean.\n",
      "\n",
      "Y:\n",
      "And shall have so?\n",
      "\n",
      "GLOUCESTER:\n",
      "I say, sir? were thee, sis long, I will;\n",
      "To most you she negs newithin our voice.\n",
      "\n",
      "GLOUCESTER: IO:\n",
      "I know thousand withink. Was I kill of thou see too join, what like soul,\n",
      "I would in you aring my chard\n",
      "And pation, vise, for me doubt lovest like.\n",
      "But--what savest for Iuch'd ward you,--\n",
      "All in the women thou scarch'd with in two-mon again.\n",
      "\n",
      "GLOUCESTER: balike of ther,--ARGILI:\n",
      "\n",
      "not so, mock of Richard too, should be grace of great--\n",
      "And exality of my gh; one own\n",
      "To reature,--which want, Law, not wink\n",
      "And for great--we back wiserabbawd.\n",
      "\n",
      "KING Richard toak: down Is they thee,--\n",
      "\n",
      "Your ream in theer spen all with his cool-wakers frial:\n",
      "And, for we much beitter? is is these what ever\n",
      "And kind men soon of my life?\n",
      "\n",
      "GLOUCESTER:\n",
      "Come horse, go wake: men, I mean, whose did year,\n",
      "If those wise, for me kness.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "\n",
      "\n",
      "fome,\n",
      "Chriss,--which I shall feeter teen with thee.\n",
      "\n",
      "GLOUCESTER:\n",
      "These, let thy lives your countering natives,\n",
      "Take this directorther blood, let for where in:\n",
      "I do, is there live, you of this life.\n",
      "\n",
      "GEORGE:\n",
      "I prated Tiew; for a word with stand wish pers cheerfuour parture:\n",
      "For where is speak.\n",
      "\n",
      "GREEN: now Is Edwant, of spoils, Lat make son:\n",
      "'Tis other! was with of his sanswer of Parison,\n",
      "And how was, part to child, welcome dover,\n",
      "Let 's with all wontracefully it:\n",
      "For of wlangue.\n",
      "\n",
      "Fir ever goveren light, sir: your will go wise,\n",
      "Ere of Govern wer to the mleasure livereign.\n",
      "\n",
      "ANGELO:\n",
      "And like consess my wife,\n",
      "Of what learness will with you, I know\n",
      "This like mer want. What's with all in year,\n",
      "Inseen his ago. be so, weere may of your mon\n",
      "Wear him with prisoner from Lon, all.\n",
      "\n",
      "GLOUCESTER:\n",
      "I schard you, babe, that shall sess new theer patience:\n",
      "I will hope this: you, all my learn,\n",
      "To storm traisery with speech arged so gals.\n",
      "\n",
      "GLOUCESTER:\n",
      "I slive you, to see:\n",
      "Mare you, it good go was be great man\n",
      "\n",
      "\n",
      "f said, the love\n",
      "Tider's dusty Messength temper all of Richoofs,\n",
      "Of whence will go ward presenation Deserved;\n",
      "Which oldst clany, then thy Richard,\n",
      "And not now were in curre swears to comes,--\n",
      "Or-house! Where do res believe, and desire non,\n",
      "Whose fecboy ofught. Set trin, 'tis safe,\n",
      "Alack bear the woon, and here I was\n",
      "I stain the was to dear; her, but never for hear men. And with--\n",
      "\n",
      "Where, straitor:\n",
      "What names-safe of thou sconfimorous,\n",
      "Whose him from my word: wofure wons,\n",
      "But would were, being some for his lord?\n",
      "\n",
      "ROMEO:\n",
      "What'st so? is like? what, And poor wron thes?\n",
      "And world, for hurly, and what with Romeo\n",
      "If though make gly speak upon where, howise, it for slew men\n",
      "And your jamen's pleasure of the murder thes:\n",
      "And hat being was speak my harge him me,\n",
      "My was tears, for evereignhome, whose make like a live,\n",
      "I know.\n",
      "In for her with count, belike you,--I prisoZA foot--\n",
      "\n",
      "DUO: low our of the from the world, Lord Mak.\n",
      "\n",
      "GLOUCESTER:\n",
      "I say--O, mide up cockord; and you sand you,\n",
      "And her enjoison \n",
      "\n",
      "\n",
      "f spoke to thee my years\n",
      "BurghewinghRASTIANA:\n",
      "Armhalf that?\n",
      "\n",
      "ARIT:\n",
      "Ay, that, his too?\n",
      "\n",
      "BRUTUS:\n",
      "There's or own the our send to the loves long.\n",
      "\n",
      "DERBY:\n",
      "The consce he go you see, mog.\n",
      "\n",
      "GLOUCESTER:\n",
      "Sit dowrs, it ho, he\n",
      "due first perform along thus? of pace, so belitteer\n",
      "Again that your make less. Welcome how, for wron old me\n",
      "Herefor sake.\n",
      "\n",
      "GLOUCESTER: 'er wager say too, well: mages will, good\n",
      "And Morely good spent nexarcius. I usure al, was now his tereign of her him hath\n",
      "\n",
      "Murior:\n",
      "My word, will his consens prince with your from me life,\n",
      "To saven his sake him else love him himself ancy\n",
      "Worth speak his againsway son with hath ness, bell,\n",
      "Whose for your ise eye harms, fastining eason\n",
      "Woth season With pack-speech hard-for dagger--\n",
      "As Above for law, and fall of spenment the walls,\n",
      "There hath man your row liverses been is cons; for and tear--Wick,\n",
      "That like threward will be am neasure.\n",
      "\n",
      "SICIRGH:\n",
      "And obabe. But\n",
      "Of my reamsny, for is tongue! will you have world your heart!\n",
      "\n",
      "PETRUCHIO:\n",
      "Sir cabout.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_generator.update_params(max_length=1000, num_samples=5)\n",
    "mlp_generator.generate()\n",
    "mlp_generator.print_outputs() # from those three outputs, print the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c96d9",
   "metadata": {
    "papermill": {
     "duration": 0.141412,
     "end_time": "2024-07-10T20:16:47.326029",
     "exception": false,
     "start_time": "2024-07-10T20:16:47.184617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463522d3",
   "metadata": {
    "papermill": {
     "duration": 0.140286,
     "end_time": "2024-07-10T20:16:47.608496",
     "exception": false,
     "start_time": "2024-07-10T20:16:47.468210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0e1ea",
   "metadata": {
    "papermill": {
     "duration": 0.141233,
     "end_time": "2024-07-10T20:16:47.889061",
     "exception": false,
     "start_time": "2024-07-10T20:16:47.747828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592aef88",
   "metadata": {
    "papermill": {
     "duration": 0.146327,
     "end_time": "2024-07-10T20:16:48.180393",
     "exception": false,
     "start_time": "2024-07-10T20:16:48.034066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ae431",
   "metadata": {
    "papermill": {
     "duration": 0.14894,
     "end_time": "2024-07-10T20:16:48.478187",
     "exception": false,
     "start_time": "2024-07-10T20:16:48.329247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5339176,
     "sourceId": 8871363,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 711.995632,
   "end_time": "2024-07-10T20:16:50.049055",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-10T20:04:58.053423",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
